# Model Configurations
lr: 0.001  # Learning rate for the optimizer
weight_decay: 0.2  # Weight decay for regularization
p: 43  # Prime number used for modular arithmetic operations
d_model: 43  # Dimensionality of model embeddings
fn_name: 'add'  # Name of the function to be learned ('add', 'subtract', etc.)
frac_train: 0.25  # Fraction of data used for training
num_epochs: 200000  # Number of training epochs
save_models: False  # Flag indicating whether to save model checkpoints
save_every: 100  # Frequency (in epochs) at which to save models
optimizer: 'AdamW' #'AdamW', SGD''

# Early stopping criteria based on test loss
stopping_thresh: -1  # Training stops if test loss falls below this value
seed: 1024  # Random seed for reproducibility

# Transformer Model Architecture Parameters
batch_style: 'full'  # Batch processing style (e.g., 'full' or mini-batch)
d_vocab: 43  # Vocabulary size, typically p for modular arithmetic
d_mlp: 128 # Dimensionality of the MLP (feedforward) layers in transformer blocks
act_type: 'ReLU'  # Activation function used in the MLP layers ('ReLU' or 'GeLU')

# Miscellaneous Parameters
take_metrics_every_n_epochs: 1000
use_ln: False